{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "작물 잎 사진으로 질병 분류.ipynb",
      "provenance": [],
      "mount_file_id": "1LC6vd1wDSbTpfnZLwX-I9HVm6EZNytJ_",
      "authorship_tag": "ABX9TyP2/0gG4ZLA70aDIkjA8xJ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhy02094/pytorch/blob/master/%EC%9E%91%EB%AC%BC_%EC%9E%8E_%EC%82%AC%EC%A7%84%EC%9C%BC%EB%A1%9C_%EC%A7%88%EB%B3%91_%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uV4uCtl0R0TY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc850e3-9560-492d-f162-2f62da28ddf5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/MyDrive/Colab Notebooks/pytorch/작물 잎 사진\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rYeAlxBIiwL",
        "outputId": "ae2c381d-0607-4c51-838e-5fdc8dec56c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/pytorch/작물 잎 사진\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq \"/content/drive/MyDrive/Colab Notebooks/pytorch/작물 잎 사진/dataset.zip의 사본\""
      ],
      "metadata": {
        "id": "h6O1FHwCItOt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 분할을 위한 폴더 생성"
      ],
      "metadata": {
        "id": "50f8EECaMQ28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BtC_dRWaHbv2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "original_dataset_dir = '/content/drive/MyDrive/Colab Notebooks/pytorch/작물 잎 사진/dataset'                    # 저장된 데이터 공간\n",
        "classes_list = os.listdir(original_dataset_dir)       # 저장된 데이터 공간에서 모든 폴더이름 가져오기\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Colab Notebooks/pytorch/작물 잎 사진/splitted'                               # 나눈 데이터를 앞으로 저장할 폴더\n",
        "os.mkdir(base_dir)                                    \n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')           # 경로(base_dir/tarin)\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for clss in classes_list :                           # 모든 폴더이름을 트레인, 검증, 테스트에 넣기\n",
        "    os.mkdir(os.path.join(train_dir, clss))\n",
        "    os.mkdir(os.path.join(validation_dir, clss))\n",
        "    os.mkdir(os.path.join(test_dir, clss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 분할과 클래스별 데이터 수 확인"
      ],
      "metadata": {
        "id": "TfSy8nmHMatZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "for clss in classes_list :\n",
        "    path = os.path.join(original_dataset_dir, clss)   #권한을 주는건데 original - clss 로 연결\n",
        "    fnames = os.listdir(path)      # 경로를 넣으니까 fnames는 저 위를 다 갖는거\n",
        "\n",
        "# 데이터 개수 분배\n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "\n",
        "    train_fnames = fnames[:train_size]\n",
        "    print('Train size(',clss,'): ', len(train_fnames))\n",
        "    for fname in train_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(train_dir, clss), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    print('Validation size(',clss,'): ', len(validation_fnames))\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, clss), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    test_fnames = fnames[(train_size + validation_size): \n",
        "                                (validation_size + train_size + test_size)]\n",
        "    \n",
        "    print('Test size(',clss,'): ', len(test_fnames))\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, clss), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7w8eivSMBq-",
        "outputId": "088342e9-52f5-44bd-fb97-ca424418c96a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size( Pepper,_bell___healthy ):  886\n",
            "Validation size( Pepper,_bell___healthy ):  295\n",
            "Test size( Pepper,_bell___healthy ):  295\n",
            "Train size( Grape___Esca_(Black_Measles) ):  829\n",
            "Validation size( Grape___Esca_(Black_Measles) ):  276\n",
            "Test size( Grape___Esca_(Black_Measles) ):  276\n",
            "Train size( Pepper,_bell___Bacterial_spot ):  598\n",
            "Validation size( Pepper,_bell___Bacterial_spot ):  199\n",
            "Test size( Pepper,_bell___Bacterial_spot ):  199\n",
            "Train size( Strawberry___healthy ):  273\n",
            "Validation size( Strawberry___healthy ):  91\n",
            "Test size( Strawberry___healthy ):  91\n",
            "Train size( Grape___Black_rot ):  708\n",
            "Validation size( Grape___Black_rot ):  236\n",
            "Test size( Grape___Black_rot ):  236\n",
            "Train size( Corn___Common_rust ):  715\n",
            "Validation size( Corn___Common_rust ):  238\n",
            "Test size( Corn___Common_rust ):  238\n",
            "Train size( Apple___Apple_scab ):  378\n",
            "Validation size( Apple___Apple_scab ):  126\n",
            "Test size( Apple___Apple_scab ):  126\n",
            "Train size( Potato___healthy ):  91\n",
            "Validation size( Potato___healthy ):  30\n",
            "Test size( Potato___healthy ):  30\n",
            "Train size( Potato___Late_blight ):  600\n",
            "Validation size( Potato___Late_blight ):  200\n",
            "Test size( Potato___Late_blight ):  200\n",
            "Train size( Cherry___healthy ):  512\n",
            "Validation size( Cherry___healthy ):  170\n",
            "Test size( Cherry___healthy ):  170\n",
            "Train size( Tomato___Bacterial_spot ):  1276\n",
            "Validation size( Tomato___Bacterial_spot ):  425\n",
            "Test size( Tomato___Bacterial_spot ):  425\n",
            "Train size( Apple___Black_rot ):  372\n",
            "Validation size( Apple___Black_rot ):  124\n",
            "Test size( Apple___Black_rot ):  124\n",
            "Train size( Cherry___Powdery_mildew ):  631\n",
            "Validation size( Cherry___Powdery_mildew ):  210\n",
            "Test size( Cherry___Powdery_mildew ):  210\n",
            "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  307\n",
            "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
            "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
            "Train size( Peach___healthy ):  216\n",
            "Validation size( Peach___healthy ):  72\n",
            "Test size( Peach___healthy ):  72\n",
            "Train size( Tomato___Early_blight ):  600\n",
            "Validation size( Tomato___Early_blight ):  200\n",
            "Test size( Tomato___Early_blight ):  200\n",
            "Train size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  3214\n",
            "Validation size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
            "Test size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
            "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  645\n",
            "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
            "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
            "Train size( Potato___Early_blight ):  600\n",
            "Validation size( Potato___Early_blight ):  200\n",
            "Test size( Potato___Early_blight ):  200\n",
            "Train size( Grape___healthy ):  253\n",
            "Validation size( Grape___healthy ):  84\n",
            "Test size( Grape___healthy ):  84\n",
            "Train size( Apple___Cedar_apple_rust ):  165\n",
            "Validation size( Apple___Cedar_apple_rust ):  55\n",
            "Test size( Apple___Cedar_apple_rust ):  55\n",
            "Train size( Corn___Northern_Leaf_Blight ):  591\n",
            "Validation size( Corn___Northern_Leaf_Blight ):  197\n",
            "Test size( Corn___Northern_Leaf_Blight ):  197\n",
            "Train size( Tomato___Septoria_leaf_spot ):  1062\n",
            "Validation size( Tomato___Septoria_leaf_spot ):  354\n",
            "Test size( Tomato___Septoria_leaf_spot ):  354\n",
            "Train size( Corn___healthy ):  697\n",
            "Validation size( Corn___healthy ):  232\n",
            "Test size( Corn___healthy ):  232\n",
            "Train size( Strawberry___Leaf_scorch ):  665\n",
            "Validation size( Strawberry___Leaf_scorch ):  221\n",
            "Test size( Strawberry___Leaf_scorch ):  221\n",
            "Train size( Tomato___Tomato_mosaic_virus ):  223\n",
            "Validation size( Tomato___Tomato_mosaic_virus ):  74\n",
            "Test size( Tomato___Tomato_mosaic_virus ):  74\n",
            "Train size( Tomato___Leaf_Mold ):  571\n",
            "Validation size( Tomato___Leaf_Mold ):  190\n",
            "Test size( Tomato___Leaf_Mold ):  190\n",
            "Train size( Tomato___Late_blight ):  1145\n",
            "Validation size( Tomato___Late_blight ):  381\n",
            "Test size( Tomato___Late_blight ):  381\n",
            "Train size( Tomato___healthy ):  954\n",
            "Validation size( Tomato___healthy ):  318\n",
            "Test size( Tomato___healthy ):  318\n",
            "Train size( Tomato___Spider_mites Two-spotted_spider_mite ):  1005\n",
            "Validation size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
            "Test size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
            "Train size( Apple___healthy ):  987\n",
            "Validation size( Apple___healthy ):  329\n",
            "Test size( Apple___healthy ):  329\n",
            "Train size( Tomato___Target_Spot ):  842\n",
            "Validation size( Tomato___Target_Spot ):  280\n",
            "Test size( Tomato___Target_Spot ):  280\n",
            "Train size( Peach___Bacterial_spot ):  1378\n",
            "Validation size( Peach___Bacterial_spot ):  459\n",
            "Test size( Peach___Bacterial_spot ):  459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "베이스라인 모델 학습을 위한 준비"
      ],
      "metadata": {
        "id": "ou4VkJ2rRJSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
        "\n",
        "print('Current cuda device is', DEVICE)\n",
        "\n",
        "# 오 코랩은 cuda 로 돌리는게 가능하다!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqC7xGo6MDvz",
        "outputId": "5fe780b2-cc8a-47a7-aa06-9269b37ce71e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current cuda device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "EPOCH = 30\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder   # 계층적인 폴더 구조를 가지고 있는 데이터셋을 불러올때 사용\n",
        "\n",
        "transform_base = transforms.Compose([transforms.Resize((64,64)),\n",
        "                                     transforms.ToTensor()])\n",
        "# transforms.Compose (이미지 전처리 , Augmentation 에 사용)\n",
        "# augmentation을 통해 좌우 반전, 발긱 조절, 이미지 확대가 가능. 여기선 64,64 로 조정하고 Totensor로 변환하면서 0~1 정규화\n",
        "\n",
        "train_dataset = ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/pytorch/작물 잎 사진/splitted/train',transform = transform_base)\n",
        "\n",
        "val_dataset = ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/pytorch/작물 잎 사진/splitted/val',\n",
        "                          transform = transform_base)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size = BATCH_SIZE,\n",
        "                                           shuffle = True,\n",
        "                                           num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                         batch_size = BATCH_SIZE,\n",
        "                                         shuffle = True,\n",
        "                                         num_workers = 4) \n",
        "# num_workers 는 gpu 얼마나 할당할지."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtG3kOwpTtix",
        "outputId": "c3eb473f-d49d-4a45-c3d9-6b0c14893084"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    # 속성 생성\n",
        "    def __init__(self):      \n",
        "\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding = 1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv2 = nn.Conv2d(32,64,3, padding = 1)\n",
        "        self.conv3 = nn.Conv2d(64,64,3, padding = 1)\n",
        "\n",
        "        self.fc1 = nn.Linear(4096, 512)\n",
        "        self.fc2 = nn.Linear(512, 33)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.dropout(x, p = 0.25, training = self.training)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.dropout(x, p=0.25, training= self.training)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.dropout(x, p = 0.25, training = self.training)\n",
        "\n",
        "        x = x.view(-1,4096)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training = self.training)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x,dim=1)\n",
        "\n",
        "model_base = Net().to(DEVICE)\n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "QB7u7z6EMCJF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[모델 학습을 위한 학습]"
      ],
      "metadata": {
        "id": "xpCoIDs4_k77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):  #target이 뭔가 싶긴한데... 아마 폴더로 설정되나 보다.\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad() #이전 배치의 그래디언트값이 옵티마이저에 저장되어 있으므로 초기화\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "GtCWHsjEMCQj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[모델 평가를 위한 학습]"
      ],
      "metadata": {
        "id": "1oAp5gCxBWim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0  #미니 배치별로 Loss를 합산해서 저장할 변수\n",
        "    correct = 0   #올바르게 예측한 데이터 수를 세는 변수\n",
        "\n",
        "    with torch.no_grad():   # 모델 평가에서는 parameter를 업데이트 하지 않아야 한다. #이 메소드는 parameter업데이트를 중단\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "\n",
        "            test_loss += F.cross_entropy(output,\n",
        "                                         target, reduction = 'sum').item()\n",
        "            \n",
        "            pred = output.max(1, keepdim= True)[1] #33개의 클래스에 속할 각각의 확률값 출력\n",
        "\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "                                         \n",
        "\n"
      ],
      "metadata": {
        "id": "AlHlZvq8BWMt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[모델 학습 실행]"
      ],
      "metadata": {
        "id": "YUs1rwIXD111"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def train_baseline(model, train_loader, val_loader,\n",
        "                   optimizer, num_epochs =30):\n",
        "    best_acc = 0 # 정확도 저장 변수\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    for epoch in range(1, num_epochs +1):\n",
        "        since = time.time()\n",
        "        train(model, train_loader, optimizer)\n",
        "        train_loss, train_acc = evaluate(model, train_loader)\n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        time_elapsed = time.time() - since # 한 epcoh마다 걸린 시간\n",
        "        print('-----------------epoch {} ----------------------'.format(epoch))\n",
        "\n",
        "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'\n",
        "                                    .format(train_loss, train_acc))\n",
        "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'\n",
        "                                    .format(val_loss, val_acc))\n",
        "        print('Completed in {:.0f}m {:.0f}s'\n",
        "                                    .format(train_loss, train_acc))\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "8qHVv3Y6MCVF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base = train_baseline(model_base, train_loader,\n",
        "                      val_loader, optimizer, EPOCH)\n",
        "\n",
        "torch.save(base, 'baseline.pt')"
      ],
      "metadata": {
        "id": "cRRJTwwRMCXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26f851b-1910-46ba-b02a-53787f94fc11"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------epoch 1 ----------------------\n",
            "train Loss: 1.6396, Accuracy: 54.02%\n",
            "val Loss: 1.6605, Accuracy: 53.21%\n",
            "Completed in 2m 54s\n",
            "-----------------epoch 2 ----------------------\n",
            "train Loss: 0.9077, Accuracy: 73.07%\n",
            "val Loss: 0.9490, Accuracy: 72.19%\n",
            "Completed in 1m 73s\n",
            "-----------------epoch 3 ----------------------\n",
            "train Loss: 0.7060, Accuracy: 78.39%\n",
            "val Loss: 0.7735, Accuracy: 76.67%\n",
            "Completed in 1m 78s\n",
            "-----------------epoch 4 ----------------------\n",
            "train Loss: 0.5359, Accuracy: 83.40%\n",
            "val Loss: 0.6166, Accuracy: 81.24%\n",
            "Completed in 1m 83s\n",
            "-----------------epoch 5 ----------------------\n",
            "train Loss: 0.4456, Accuracy: 85.90%\n",
            "val Loss: 0.5342, Accuracy: 84.09%\n",
            "Completed in 0m 86s\n",
            "-----------------epoch 6 ----------------------\n",
            "train Loss: 0.3975, Accuracy: 87.85%\n",
            "val Loss: 0.4963, Accuracy: 85.14%\n",
            "Completed in 0m 88s\n",
            "-----------------epoch 7 ----------------------\n",
            "train Loss: 0.3427, Accuracy: 89.45%\n",
            "val Loss: 0.4516, Accuracy: 86.23%\n",
            "Completed in 0m 89s\n",
            "-----------------epoch 8 ----------------------\n",
            "train Loss: 0.2899, Accuracy: 91.33%\n",
            "val Loss: 0.4076, Accuracy: 87.77%\n",
            "Completed in 0m 91s\n",
            "-----------------epoch 9 ----------------------\n",
            "train Loss: 0.2525, Accuracy: 92.22%\n",
            "val Loss: 0.3742, Accuracy: 88.90%\n",
            "Completed in 0m 92s\n",
            "-----------------epoch 10 ----------------------\n",
            "train Loss: 0.2538, Accuracy: 91.99%\n",
            "val Loss: 0.3966, Accuracy: 88.11%\n",
            "Completed in 0m 92s\n",
            "-----------------epoch 11 ----------------------\n",
            "train Loss: 0.1978, Accuracy: 94.64%\n",
            "val Loss: 0.3353, Accuracy: 90.57%\n",
            "Completed in 0m 95s\n",
            "-----------------epoch 12 ----------------------\n",
            "train Loss: 0.1801, Accuracy: 94.44%\n",
            "val Loss: 0.3280, Accuracy: 90.54%\n",
            "Completed in 0m 94s\n",
            "-----------------epoch 13 ----------------------\n",
            "train Loss: 0.1597, Accuracy: 95.40%\n",
            "val Loss: 0.3068, Accuracy: 90.98%\n",
            "Completed in 0m 95s\n",
            "-----------------epoch 14 ----------------------\n",
            "train Loss: 0.1602, Accuracy: 95.35%\n",
            "val Loss: 0.3067, Accuracy: 91.15%\n",
            "Completed in 0m 95s\n",
            "-----------------epoch 15 ----------------------\n",
            "train Loss: 0.1348, Accuracy: 96.04%\n",
            "val Loss: 0.2940, Accuracy: 91.79%\n",
            "Completed in 0m 96s\n",
            "-----------------epoch 16 ----------------------\n",
            "train Loss: 0.1299, Accuracy: 96.41%\n",
            "val Loss: 0.2952, Accuracy: 91.73%\n",
            "Completed in 0m 96s\n",
            "-----------------epoch 17 ----------------------\n",
            "train Loss: 0.1159, Accuracy: 96.63%\n",
            "val Loss: 0.2793, Accuracy: 92.01%\n",
            "Completed in 0m 97s\n",
            "-----------------epoch 18 ----------------------\n",
            "train Loss: 0.1048, Accuracy: 97.27%\n",
            "val Loss: 0.2694, Accuracy: 92.43%\n",
            "Completed in 0m 97s\n",
            "-----------------epoch 19 ----------------------\n",
            "train Loss: 0.0993, Accuracy: 97.32%\n",
            "val Loss: 0.2650, Accuracy: 92.78%\n",
            "Completed in 0m 97s\n",
            "-----------------epoch 20 ----------------------\n",
            "train Loss: 0.0906, Accuracy: 97.44%\n",
            "val Loss: 0.2654, Accuracy: 92.28%\n",
            "Completed in 0m 97s\n",
            "-----------------epoch 21 ----------------------\n",
            "train Loss: 0.0941, Accuracy: 97.52%\n",
            "val Loss: 0.2841, Accuracy: 92.05%\n",
            "Completed in 0m 98s\n",
            "-----------------epoch 22 ----------------------\n",
            "train Loss: 0.0950, Accuracy: 97.51%\n",
            "val Loss: 0.2766, Accuracy: 92.13%\n",
            "Completed in 0m 98s\n",
            "-----------------epoch 23 ----------------------\n",
            "train Loss: 0.0752, Accuracy: 97.96%\n",
            "val Loss: 0.2616, Accuracy: 92.66%\n",
            "Completed in 0m 98s\n",
            "-----------------epoch 24 ----------------------\n",
            "train Loss: 0.0781, Accuracy: 97.76%\n",
            "val Loss: 0.2831, Accuracy: 92.04%\n",
            "Completed in 0m 98s\n",
            "-----------------epoch 25 ----------------------\n",
            "train Loss: 0.0595, Accuracy: 98.54%\n",
            "val Loss: 0.2495, Accuracy: 93.30%\n",
            "Completed in 0m 99s\n",
            "-----------------epoch 26 ----------------------\n",
            "train Loss: 0.0801, Accuracy: 97.96%\n",
            "val Loss: 0.2763, Accuracy: 92.44%\n",
            "Completed in 0m 98s\n",
            "-----------------epoch 27 ----------------------\n",
            "train Loss: 0.0625, Accuracy: 98.38%\n",
            "val Loss: 0.2588, Accuracy: 92.71%\n",
            "Completed in 0m 98s\n",
            "-----------------epoch 28 ----------------------\n",
            "train Loss: 0.0539, Accuracy: 98.83%\n",
            "val Loss: 0.2455, Accuracy: 93.32%\n",
            "Completed in 0m 99s\n",
            "-----------------epoch 29 ----------------------\n",
            "train Loss: 0.0436, Accuracy: 99.05%\n",
            "val Loss: 0.2346, Accuracy: 93.65%\n",
            "Completed in 0m 99s\n",
            "-----------------epoch 30 ----------------------\n",
            "train Loss: 0.0520, Accuracy: 98.74%\n",
            "val Loss: 0.2502, Accuracy: 93.24%\n",
            "Completed in 0m 99s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_base = transforms.Compose([transforms.Resize([64,64]),\n",
        "                                     transforms.ToTensor()])\n",
        "test_base = ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/pytorch/작물 잎 사진/splitted/test',\n",
        "                        transform=transform_base)\n",
        "test_loader_base = torch.utils.data.DataLoader(test_base,\n",
        "                                               batch_size = BATCH_SIZE, shuffle=True,\n",
        "                                               num_workers=4)"
      ],
      "metadata": {
        "id": "TrXCFH8-MCaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040a2ace-c5b7-4722-c08d-a3ade8b38a2a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline=torch.load('baseline.pt')\n",
        "baseline.eval()\n",
        "test_loss, test_accuracy = evaluate(baseline, test_loader_base)\n",
        "\n",
        "print('baseline test acc: ', test_accuracy)"
      ],
      "metadata": {
        "id": "CHGFc8CYMEx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e56034-8c43-4dfa-a311-7791bc97a6f2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline test acc:  93.87908374014269\n"
          ]
        }
      ]
    }
  ]
}